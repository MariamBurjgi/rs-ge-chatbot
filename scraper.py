import requests
import json
import time
from bs4 import BeautifulSoup

# --- áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ ---
BASE_URL = "https://infohubapi.rs.ge/api/documents"

# áƒ¡áƒ£áƒš áƒ’áƒ•áƒ˜áƒœáƒ“áƒ 100 áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜
TARGET_TOTAL = 100
# áƒ”áƒ áƒ— áƒ¯áƒ”áƒ áƒ–áƒ” áƒ•áƒ˜áƒ—áƒ®áƒáƒ•áƒ— 20-áƒ¡ (áƒ áƒáƒ› áƒ¡áƒ”áƒ áƒ•áƒ”áƒ áƒ˜ áƒáƒ  áƒ’áƒáƒ‘áƒ áƒáƒ–áƒ“áƒ”áƒ¡ - 422 áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒáƒ  áƒáƒ›áƒáƒáƒ’áƒ“áƒáƒ¡)
BATCH_SIZE = 20

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "LanguageCode": "ka",
    "Origin": "https://infohub.rs.ge",
    "Referer": "https://infohub.rs.ge/",
    "Accept": "application/json, text/plain, */*"
}

def clean_html(html_content):
    if not html_content:
        return ""
    try:
        soup = BeautifulSoup(html_content, "html.parser")
        return soup.get_text(separator="\n").strip()
    except:
        return str(html_content)

def get_document_list():
    all_documents = []
    skip = 0
    
    print(f"â³ áƒ•áƒ˜áƒ¬áƒ§áƒ”áƒ‘ {TARGET_TOTAL} áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ’áƒ áƒáƒ•áƒ”áƒ‘áƒáƒ¡ áƒœáƒáƒ¬áƒ˜áƒš-áƒœáƒáƒ¬áƒ˜áƒš...")

    while len(all_documents) < TARGET_TOTAL:
        print(f"   â†³ áƒ•áƒ˜áƒ—áƒ®áƒáƒ• áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ¡ {skip}-áƒ“áƒáƒœ {skip + BATCH_SIZE}-áƒ›áƒ“áƒ”...")
        
        params = {
            "skip": skip,
            "take": BATCH_SIZE,
            "species": "NewDocument"
        }
        
        try:
            response = requests.get(BASE_URL, headers=HEADERS, params=params)
            if response.status_code == 200:
                data = response.json().get("data", [])
                if not data:
                    print("   âš ï¸ áƒ›áƒ”áƒ¢áƒ˜ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒáƒ¦áƒáƒ  áƒáƒ áƒ˜áƒ¡.")
                    break
                
                all_documents.extend(data)
                skip += BATCH_SIZE # áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’ áƒ¯áƒ”áƒ áƒ–áƒ” áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’áƒ˜ 20-áƒ˜áƒ¡ áƒ¬áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ
                time.sleep(0.5) # áƒ¨áƒ”áƒ¡áƒ•áƒ”áƒœáƒ”áƒ‘áƒ
            else:
                print(f"âŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {response.status_code}")
                break
        except Exception as e:
            print(f"âŒ áƒ™áƒ áƒ˜áƒ¢áƒ˜áƒ™áƒ£áƒšáƒ˜ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {e}")
            break
            
    return all_documents[:TARGET_TOTAL] # áƒ–áƒ£áƒ¡áƒ¢áƒáƒ“ áƒ˜áƒ›áƒ“áƒ”áƒœáƒ¡ áƒ•áƒáƒ‘áƒ áƒ£áƒœáƒ”áƒ‘áƒ—, áƒ áƒáƒ›áƒ“áƒ”áƒœáƒ˜áƒª áƒ’áƒ•áƒ˜áƒœáƒ“áƒáƒ“áƒ

def get_document_details(doc_id):
    url = f"{BASE_URL}/{doc_id}/details-by-key?openFromSearch=false"
    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            data = response.json()
            raw_html = data.get("body") or data.get("description") or data.get("content") or ""
            return clean_html(raw_html)
        return None
    except:
        return None

def main():
    documents = get_document_list()
    
    if not documents:
        print("âŒ áƒ¡áƒ˜áƒ áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜áƒ.")
        return

    print(f"\nâœ… áƒ¡áƒ£áƒš áƒ¨áƒ”áƒ’áƒ áƒáƒ•áƒ“áƒ {len(documents)} áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ¡áƒáƒ—áƒáƒ£áƒ áƒ˜. áƒ•áƒ˜áƒ¬áƒ§áƒ”áƒ‘ áƒ¨áƒ˜áƒœáƒáƒáƒ áƒ¡áƒ˜áƒ¡ áƒ¬áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒáƒ¡...\n")

    final_data = []

    for i, doc in enumerate(documents):
        doc_id = doc.get("uniqueKey")
        title = doc.get("title")
        doc_number = doc.get("documentNumber")
        
        if not title:
            if doc_number:
                title = f"áƒ‘áƒ áƒ«áƒáƒœáƒ”áƒ‘áƒ â„–{doc_number}"
            else:
                title = "áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜ (áƒ¡áƒáƒ—áƒáƒ£áƒ áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”)"
            
        print(f"[{i+1}/{len(documents)}] áƒ•áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘: {str(title)[:50]}...")
        
        if not doc_id:
            continue

        full_text = get_document_details(doc_id)
        
        if full_text and len(full_text) > 10: 
            if title == "áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜ (áƒ¡áƒáƒ—áƒáƒ£áƒ áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”)":
                title = full_text[:60].replace("\n", " ") + "..."

            entry = {
                "id": doc_id,
                "title": title,
                "date": doc.get("receiptDate", ""),
                "content": full_text,
                "url": f"https://infohub.rs.ge/ka/workspace/document/{doc_id}",
                "type": doc.get("typeName", "áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜")
            }
            final_data.append(entry)
        
        time.sleep(0.1)

    if final_data:
        with open("data.json", "w", encoding="utf-8") as f:
            json.dump(final_data, f, ensure_ascii=False, indent=4)
        print(f"\nğŸ‰ áƒ’áƒ˜áƒšáƒáƒªáƒáƒ•! {len(final_data)} áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒ¨áƒ”áƒœáƒáƒ®áƒ£áƒšáƒ˜áƒ!")
    else:
        print("\nâš ï¸ áƒ¡áƒáƒ›áƒ¬áƒ£áƒ®áƒáƒ áƒáƒ“, áƒ•áƒ”áƒ áƒªáƒ”áƒ áƒ—áƒ˜ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜ áƒ•áƒ”áƒ  áƒáƒ›áƒáƒ•áƒ˜áƒ¦áƒ”.")

if __name__ == "__main__":
    main()
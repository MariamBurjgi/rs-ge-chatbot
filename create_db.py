import json
import os
import time
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

# áƒ’áƒáƒ áƒ”áƒ›áƒáƒ¡ áƒªáƒ•áƒšáƒáƒ“áƒ”áƒ‘áƒ˜áƒ¡ áƒ¬áƒáƒ™áƒ˜áƒ—áƒ®áƒ•áƒ
load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    print("âŒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: OPENAI_API_KEY áƒ•áƒ”áƒ  áƒ•áƒ˜áƒáƒáƒ•áƒ” .env áƒ¤áƒáƒ˜áƒšáƒ¨áƒ˜!")
    exit()

# áƒ“áƒáƒ›áƒ®áƒ›áƒáƒ áƒ” áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ, áƒ áƒáƒ›áƒ”áƒšáƒ˜áƒª áƒ¡áƒ˜áƒáƒ¡ áƒáƒáƒ¢áƒáƒ áƒ áƒœáƒáƒ¬áƒ˜áƒšáƒ”áƒ‘áƒáƒ“ (Batches) áƒ§áƒáƒ¤áƒ¡
def batch_process(iterable, n=1):
    l = len(iterable)
    for ndx in range(0, l, n):
        yield iterable[ndx:min(ndx + n, l)]

def create_vector_db():
    print("ğŸ“‚ áƒ•áƒ˜áƒ¬áƒ§áƒ”áƒ‘ data.json-áƒ˜áƒ¡ áƒ¬áƒáƒ™áƒ˜áƒ—áƒ®áƒ•áƒáƒ¡...")
    
    try:
        with open("data.json", "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        print("âŒ data.json áƒ•áƒ”áƒ  áƒ•áƒ˜áƒáƒáƒ•áƒ”!")
        return

    documents = []
    
    # áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ›áƒáƒ›áƒ–áƒáƒ“áƒ”áƒ‘áƒ
    for entry in data:
        content = entry.get("content", "")
        metadata = {
            "source": entry.get("url", ""),
            "title": entry.get("title", "áƒ¡áƒáƒ—áƒáƒ£áƒ áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”"),
            "date": entry.get("date", "")
        }
        if content:
            doc = Document(page_content=content, metadata=metadata)
            documents.append(doc)

    print(f"âœ… áƒ¬áƒáƒ•áƒ˜áƒ™áƒ˜áƒ—áƒ®áƒ” {len(documents)} áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜.")

    # áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ“áƒáƒ­áƒ áƒ
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    
    splits = text_splitter.split_documents(documents)
    print(f"ğŸ”ª áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜ áƒ“áƒáƒ•áƒ­áƒ”áƒ áƒ˜ {len(splits)} áƒáƒáƒ¢áƒáƒ áƒ áƒœáƒáƒ¬áƒ˜áƒšáƒáƒ“.")

    # --- áƒáƒ¥ áƒáƒ áƒ˜áƒ¡ áƒªáƒ•áƒšáƒ˜áƒšáƒ”áƒ‘áƒ: áƒœáƒáƒ¬áƒ˜áƒš-áƒœáƒáƒ¬áƒ˜áƒš áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ ---
    print("ğŸ§  áƒ•áƒ˜áƒ¬áƒ§áƒ”áƒ‘ áƒ‘áƒáƒ–áƒ˜áƒ¡ áƒ¨áƒ”áƒ•áƒ¡áƒ”áƒ‘áƒáƒ¡ (áƒœáƒáƒ¬áƒ˜áƒš-áƒœáƒáƒ¬áƒ˜áƒš)...")
    
    persist_directory = "./chroma_fixed"
    embedding_function = OpenAIEmbeddings()
    
    # áƒ•áƒ¥áƒ›áƒœáƒ˜áƒ— áƒªáƒáƒ áƒ˜áƒ”áƒš áƒ‘áƒáƒ–áƒáƒ¡ áƒáƒœ áƒ•áƒ˜áƒ¦áƒ”áƒ‘áƒ— áƒáƒ áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ¡
    vectorstore = Chroma(
        persist_directory=persist_directory, 
        embedding_function=embedding_function
    )
    
    # áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ¡ áƒ•áƒ£áƒ¨áƒ•áƒ”áƒ‘áƒ— 40-40 áƒœáƒáƒ¬áƒ˜áƒšáƒáƒ“ (áƒ áƒáƒ› áƒšáƒ˜áƒ›áƒ˜áƒ¢áƒ¡ áƒáƒ  áƒ’áƒáƒ“áƒáƒ•áƒªáƒ“áƒ”áƒ—)
    batch_size = 40
    total_batches = (len(splits) + batch_size - 1) // batch_size

    for i, batch in enumerate(batch_process(splits, batch_size)):
        print(f"   â³ áƒ•áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘ áƒœáƒáƒ¬áƒ˜áƒšáƒ¡ {i+1}/{total_batches}...")
        try:
            vectorstore.add_documents(documents=batch)
            time.sleep(0.5) # áƒáƒáƒ¢áƒáƒ áƒ áƒáƒáƒ£áƒ–áƒ áƒ¡áƒ£áƒœáƒ—áƒ¥áƒ•áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
        except Exception as e:
            print(f"âš ï¸ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒáƒ› áƒœáƒáƒ¬áƒ˜áƒšáƒ–áƒ”: {e}")

    print(f"ğŸ‰ áƒ‘áƒáƒ–áƒ áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ¨áƒ”áƒ˜áƒ¥áƒ›áƒœáƒ/áƒ’áƒáƒœáƒáƒ®áƒšáƒ“áƒ '{persist_directory}' áƒ¡áƒáƒ¥áƒáƒ¦áƒáƒšáƒ“áƒ”áƒ¨áƒ˜!")

if __name__ == "__main__":
    create_vector_db()
import streamlit as st
import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

# áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ 
load_dotenv()

st.set_page_config(page_title="InfoHub AI áƒáƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒœáƒ¢áƒ˜", page_icon="ğŸ¤–")

@st.cache_resource
def load_chain():
    if not os.getenv("OPENAI_API_KEY"):
        st.error("âŒ API áƒ’áƒáƒ¡áƒáƒ¦áƒ”áƒ‘áƒ˜ áƒ•áƒ”áƒ  áƒ•áƒ˜áƒáƒáƒ•áƒ”! áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ¨áƒ”áƒáƒ›áƒáƒ¬áƒ›áƒáƒ— .env áƒ¤áƒáƒ˜áƒšáƒ˜.")
        return None

    # áƒ¨áƒ”áƒªáƒ•áƒáƒšáƒ” áƒ¡áƒáƒ®áƒ”áƒšáƒ˜
    embedding_function = OpenAIEmbeddings()
    vectorstore = Chroma(persist_directory="./chroma_fixed", embedding_function=embedding_function)
    #  k=10 (áƒ£áƒ¤áƒ áƒ áƒ›áƒ”áƒ¢ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ¡ áƒ’áƒáƒ“áƒáƒ®áƒ”áƒ“áƒáƒ•áƒ¡)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 50})

    llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

    template = """
    áƒ¨áƒ”áƒœ áƒ®áƒáƒ  áƒ¨áƒ”áƒ›áƒáƒ¡áƒáƒ•áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒáƒ›áƒ¡áƒáƒ®áƒ£áƒ áƒ˜áƒ¡ (RS.GE) áƒ•áƒ˜áƒ áƒ¢áƒ£áƒáƒšáƒ£áƒ áƒ˜ áƒáƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒœáƒ¢áƒ˜.
    áƒ¨áƒ”áƒœáƒ˜ áƒ›áƒ˜áƒ–áƒáƒœáƒ˜áƒ áƒ“áƒáƒ”áƒ®áƒ›áƒáƒ áƒ áƒ›áƒáƒ›áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ”áƒšáƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ˜áƒ”áƒ‘áƒáƒ¨áƒ˜.
    
    áƒ˜áƒœáƒ¡áƒ¢áƒ áƒ£áƒ¥áƒªáƒ˜áƒ:
    áƒ¥áƒ•áƒ”áƒ›áƒáƒ— áƒ›áƒáƒªáƒ”áƒ›áƒ£áƒšáƒ˜áƒ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒœáƒáƒ¬áƒ§áƒ•áƒ”áƒ¢áƒ”áƒ‘áƒ˜. áƒ›áƒáƒ«áƒ”áƒ‘áƒœáƒ” áƒáƒáƒ¡áƒ£áƒ®áƒ˜ áƒáƒ› áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ”áƒ‘áƒ¨áƒ˜.
    áƒ—áƒ£ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ áƒ”áƒ®áƒ”áƒ‘áƒ áƒ™áƒáƒœáƒ™áƒ áƒ”áƒ¢áƒ£áƒš áƒœáƒáƒ›áƒ”áƒ áƒ¡ (áƒ›áƒáƒ’: 2926), áƒáƒ£áƒªáƒ˜áƒšáƒ”áƒ‘áƒšáƒáƒ“ áƒ›áƒáƒ«áƒ”áƒ‘áƒœáƒ” áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ¨áƒ˜, áƒ¡áƒáƒ“áƒáƒª áƒ”áƒ¡ áƒœáƒáƒ›áƒ”áƒ áƒ˜ áƒ¬áƒ”áƒ áƒ˜áƒ.
    
    áƒ—áƒ£ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ¨áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡, áƒ—áƒ¥áƒ•áƒ˜: "áƒ¡áƒáƒ›áƒ¬áƒ£áƒ®áƒáƒ áƒáƒ“, áƒáƒ› áƒ¡áƒáƒ™áƒ˜áƒ—áƒ®áƒ–áƒ” áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ áƒ›áƒáƒ¬áƒáƒ“áƒ”áƒ‘áƒ£áƒš áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ¨áƒ˜ áƒáƒ  áƒ˜áƒ«áƒ”áƒ‘áƒœáƒ”áƒ‘áƒ."
    
    áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜:
    {context}
    
    áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ: {question}
    
    áƒáƒáƒ¡áƒ£áƒ®áƒ˜ (áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒáƒ“):
    """
    
    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )
    
    return qa_chain

# --- UI ---

st.title("ğŸ¤– InfoHub AI áƒ™áƒáƒœáƒ¡áƒ£áƒšáƒ¢áƒáƒœáƒ¢áƒ˜")
st.markdown("áƒ“áƒáƒ¡áƒ•áƒ˜ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ RS.GE-áƒ¡ áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ“áƒáƒœ.")

chain = load_chain()

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("áƒ“áƒáƒ¡áƒ•áƒ˜ áƒ™áƒ˜áƒ—áƒ®áƒ•áƒ..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if chain:
            with st.spinner("áƒ•áƒ”áƒ«áƒ”áƒ‘ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒáƒ¡..."):
                try:
                    response = chain.invoke({"query": prompt})
                    answer = response["result"]
                    source_docs = response["source_documents"]
                    
                    st.markdown(answer)
                    
                    if source_docs:
                        st.markdown("---")
                        st.caption("ğŸ“š **áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ áƒ¬áƒ§áƒáƒ áƒáƒ”áƒ‘áƒ˜:**")
                        unique_sources = set()
                        for doc in source_docs:
                            source_url = doc.metadata.get("source", "#")
                            source_title = doc.metadata.get("title", "áƒ“áƒáƒ™áƒ£áƒ›áƒ”áƒœáƒ¢áƒ˜")
                            
                            #  áƒ¤áƒ˜áƒšáƒ¢áƒ áƒ˜: áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ£áƒœáƒ˜áƒ™áƒáƒšáƒ£áƒ áƒ˜ áƒ¡áƒáƒ—áƒáƒ£áƒ áƒ”áƒ‘áƒ˜ áƒ’áƒáƒ›áƒáƒ˜áƒ¢áƒáƒœáƒáƒ¡
                            if source_title not in unique_sources:
                                st.markdown(f"- [{source_title}]({source_url})")
                                unique_sources.add(source_title)
                    
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                    
                except Exception as e:
                    st.error(f"áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {e}")
        else:
            st.error("áƒ¡áƒ˜áƒ¡áƒ¢áƒ”áƒ›áƒ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ›áƒ–áƒáƒ“.")